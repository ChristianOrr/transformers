{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Language Model\n",
    "\n",
    "We will build an autoregressive bigram language model for predicting the next token in a sequence of text. The shakespeare_char dataset will be used for this demonstration, which can be found in the data folder.  \n",
    "\n",
    "Bigram is a probabilistic model. It uses the previous token in the sequence to determine the probabilities of the next tokens occuring. Then the next token is sampled using the next tokens probabilities.  \n",
    "\n",
    "The n-gram models are a more general case of the bigram model. They differ from bigram in that they use the last n-1 tokens in the sequence instead of just the last word. This enables them to see further back in the sentence to make their prediction. \n",
    "\n",
    "### References:\n",
    "- [GPT colab notebook](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing)\n",
    "- [Video: simplest baseline: bigram language model, loss, generation](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1331s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from jax import value_and_grad\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1,115,394\n",
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "# download the tiny shakespeare dataset\n",
    "input_file_path = os.path.join('./data/shakespeare_char/input.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")\n",
    "\n",
    "# get all the unique characters that occur in this text\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 1,003,854 tokens\n",
      "val has 111,540 tokens\n"
     ]
    }
   ],
   "source": [
    "# create the train and test splits\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode both to integers\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(decode(train_ids[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(data, rng_key, batch_size, block_size):\n",
    "    \"\"\"\n",
    "    Extracts a random batch of input and target data\n",
    "    Args:\n",
    "        data: An array of all the data's token ID's.\n",
    "        rng_key: Random number generator key.\n",
    "        batch_size: Number of parallel batches.\n",
    "        block_size: Maximum time length for the token sequence.\n",
    "    Returns:\n",
    "        Input token ID's and target token ID's.\n",
    "    \"\"\"\n",
    "    ix = random.randint(key=rng_key, shape=(batch_size, ), minval=0, maxval=len(data) - block_size)\n",
    "    x = jnp.stack([data[i:i+block_size] for i in ix])\n",
    "    y = jnp.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "(4, 8)\n",
      "[[ 1 41 53 51 51 39 52 42]\n",
      " [47 41 46  1 40 63  1 58]\n",
      " [43  1 58 53  1 57 39 60]\n",
      " [58 43 56  5 42  1 46 47]]\n",
      "targets:\n",
      "(4, 8)\n",
      "[[41 53 51 51 39 52 42 43]\n",
      " [41 46  1 40 63  1 58 46]\n",
      " [ 1 58 53  1 57 39 60 43]\n",
      " [43 56  5 42  1 46 47 57]]\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch(train_ids, rng_key, batch_size, block_size)\n",
    "\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [1] the target: 41\n",
      "when input is [1, 41] the target: 53\n",
      "when input is [1, 41, 53] the target: 51\n",
      "when input is [1, 41, 53, 51] the target: 51\n",
      "when input is [1, 41, 53, 51, 51] the target: 39\n",
      "when input is [1, 41, 53, 51, 51, 39] the target: 52\n",
      "when input is [1, 41, 53, 51, 51, 39, 52] the target: 42\n",
      "when input is [1, 41, 53, 51, 51, 39, 52, 42] the target: 43\n",
      "when input is [47] the target: 41\n",
      "when input is [47, 41] the target: 46\n",
      "when input is [47, 41, 46] the target: 1\n",
      "when input is [47, 41, 46, 1] the target: 40\n",
      "when input is [47, 41, 46, 1, 40] the target: 63\n",
      "when input is [47, 41, 46, 1, 40, 63] the target: 1\n",
      "when input is [47, 41, 46, 1, 40, 63, 1] the target: 58\n",
      "when input is [47, 41, 46, 1, 40, 63, 1, 58] the target: 46\n",
      "when input is [43] the target: 1\n",
      "when input is [43, 1] the target: 58\n",
      "when input is [43, 1, 58] the target: 53\n",
      "when input is [43, 1, 58, 53] the target: 1\n",
      "when input is [43, 1, 58, 53, 1] the target: 57\n",
      "when input is [43, 1, 58, 53, 1, 57] the target: 39\n",
      "when input is [43, 1, 58, 53, 1, 57, 39] the target: 60\n",
      "when input is [43, 1, 58, 53, 1, 57, 39, 60] the target: 43\n",
      "when input is [58] the target: 43\n",
      "when input is [58, 43] the target: 56\n",
      "when input is [58, 43, 56] the target: 5\n",
      "when input is [58, 43, 56, 5] the target: 42\n",
      "when input is [58, 43, 56, 5, 42] the target: 1\n",
      "when input is [58, 43, 56, 5, 42, 1] the target: 46\n",
      "when input is [58, 43, 56, 5, 42, 1, 46] the target: 47\n",
      "when input is [58, 43, 56, 5, 42, 1, 46, 47] the target: 57\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Uses the previous token in the sequence to \n",
    "    determine the probabilities of the next token.\n",
    "    \"\"\"\n",
    "    vocab_size: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Each token directly reads off the logits for the next token from a lookup table\n",
    "        token_embedding_table = nn.Embed(num_embeddings=self.vocab_size, features=self.vocab_size)\n",
    "        logits = token_embedding_table(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "\n",
    "variables = model.init(rng_key, xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8, 65)\n"
     ]
    }
   ],
   "source": [
    "out = model.apply(variables, xb)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=['vocab_size', 'batch_size', 'max_new_tokens'])\n",
    "def generate(variables, index_seq, rng_key, vocab_size, batch_size, max_new_tokens):\n",
    "    \"\"\"\n",
    "    Generates max_new_tokens number of new tokens, \n",
    "    given the starting sequence of tokens index_seq \n",
    "    Args:\n",
    "        variables: Bigram models parameters.\n",
    "        index_seq: Array of token indices with shape (B, T), \n",
    "            where B is the batch size and T is the time steps.\n",
    "        rng_key: Random number generator key.\n",
    "        vocab_size: Number of independent tokens in the vocabulary.\n",
    "        max_new_tokens: Maximum number of new tokens to generate\n",
    "    Returns:\n",
    "        An array of generated indices\n",
    "    \"\"\"\n",
    "    # Batched sampling function\n",
    "    batched_choice = jax.vmap(jax.random.choice)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model.apply(variables, index_seq)\n",
    "        # Focus only on the last time step\n",
    "        # Shape changes from (B, T, C) -> (B, C)\n",
    "        logits = logits[:, -1, :]\n",
    "        # Convert to probabilities using softmax\n",
    "        probs = jax.nn.softmax(logits, axis=-1)\n",
    "        # Sample a token index using probs\n",
    "        rng_key, subkey = jax.random.split(rng_key)\n",
    "        batched_key = subkey.reshape(1, -1)\n",
    "        batched_key = jnp.repeat(batched_key, batch_size, axis=0)\n",
    "        a = jnp.arange(vocab_size).reshape(1, -1)\n",
    "        a = jnp.repeat(a, batch_size, axis=0)\n",
    "        next_indexes = batched_choice(batched_key, a, p=probs)\n",
    "        next_indexes = next_indexes.reshape(batch_size, -1)\n",
    "        # Append the sampled index to the running sequence\n",
    "        index_seq = jnp.concatenate([index_seq, next_indexes], axis=1)\n",
    "    return index_seq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      "\n",
      "FeRkiTvg.,jtMwetQ\n",
      "x;;zZFeVmFgOtyYaXqu,wzhj Sfh,i3rE.,rrkHm'PDy,sja33d&;K:,EEhIeMCNl zv;wZkPlNl.lqbbL\n"
     ]
    }
   ],
   "source": [
    "index_seq = jnp.zeros(shape=(1,1), dtype=jnp.uint16)\n",
    "max_new_tokens = 100\n",
    "\n",
    "generated_indices = generate(variables, index_seq, rng_key, vocab_size, 1, max_new_tokens)\n",
    "generated_indices = list(np.array(generated_indices[0]))\n",
    "print(\"Generated text: \")\n",
    "print(decode(generated_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(variables, index_seq, labels):\n",
    "    \"\"\"\n",
    "    Calculates the cross entropy loss of \n",
    "    all batches and time steps, \n",
    "    then returns the mean.\n",
    "    Args:\n",
    "        variables: Language model parameters.\n",
    "        index_seq: Array of token indices with shape (B, T), \n",
    "            where B is the batch size and T is the time steps.\n",
    "        labels: Indexes of the next token in the sequence.\n",
    "    Returns:\n",
    "        Cross entropy loss\n",
    "    \"\"\"\n",
    "    logits = model.apply(variables, index_seq)\n",
    "\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "    # Average loss across all batches and time steps\n",
    "    loss = loss.mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adamw(learning_rate=1e-2)\n",
    "opt_state = optimizer.init(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 4.1947\n",
      "Epoch: 1, Loss: 4.1744\n",
      "Epoch: 2, Loss: 4.1583\n",
      "Epoch: 3, Loss: 4.1578\n",
      "Epoch: 4, Loss: 4.1329\n",
      "Epoch: 5, Loss: 4.1435\n",
      "Epoch: 6, Loss: 4.1164\n",
      "Epoch: 7, Loss: 4.1246\n",
      "Epoch: 8, Loss: 4.1056\n",
      "Epoch: 9, Loss: 4.0930\n",
      "Epoch: 10, Loss: 4.0616\n",
      "Epoch: 11, Loss: 4.0610\n",
      "Epoch: 12, Loss: 4.0631\n",
      "Epoch: 13, Loss: 4.0392\n",
      "Epoch: 14, Loss: 4.0296\n",
      "Epoch: 15, Loss: 4.0367\n",
      "Epoch: 16, Loss: 4.0135\n",
      "Epoch: 17, Loss: 3.9982\n",
      "Epoch: 18, Loss: 3.9788\n",
      "Epoch: 19, Loss: 3.9806\n",
      "Epoch: 20, Loss: 3.9684\n",
      "Epoch: 21, Loss: 3.9566\n",
      "Epoch: 22, Loss: 3.9539\n",
      "Epoch: 23, Loss: 3.9224\n",
      "Epoch: 24, Loss: 3.9471\n",
      "Epoch: 25, Loss: 3.9120\n",
      "Epoch: 26, Loss: 3.9014\n",
      "Epoch: 27, Loss: 3.9146\n",
      "Epoch: 28, Loss: 3.8944\n",
      "Epoch: 29, Loss: 3.8898\n",
      "Epoch: 30, Loss: 3.8620\n",
      "Epoch: 31, Loss: 3.8377\n",
      "Epoch: 32, Loss: 3.8293\n",
      "Epoch: 33, Loss: 3.8346\n",
      "Epoch: 34, Loss: 3.8517\n",
      "Epoch: 35, Loss: 3.8439\n",
      "Epoch: 36, Loss: 3.8121\n",
      "Epoch: 37, Loss: 3.7939\n",
      "Epoch: 38, Loss: 3.8129\n",
      "Epoch: 39, Loss: 3.7650\n",
      "Epoch: 40, Loss: 3.7597\n",
      "Epoch: 41, Loss: 3.7425\n",
      "Epoch: 42, Loss: 3.7499\n",
      "Epoch: 43, Loss: 3.7276\n",
      "Epoch: 44, Loss: 3.7341\n",
      "Epoch: 45, Loss: 3.7159\n",
      "Epoch: 46, Loss: 3.7060\n",
      "Epoch: 47, Loss: 3.7091\n",
      "Epoch: 48, Loss: 3.6972\n",
      "Epoch: 49, Loss: 3.6868\n",
      "Epoch: 50, Loss: 3.6809\n",
      "Epoch: 51, Loss: 3.7422\n",
      "Epoch: 52, Loss: 3.6910\n",
      "Epoch: 53, Loss: 3.6594\n",
      "Epoch: 54, Loss: 3.6731\n",
      "Epoch: 55, Loss: 3.6398\n",
      "Epoch: 56, Loss: 3.6488\n",
      "Epoch: 57, Loss: 3.6138\n",
      "Epoch: 58, Loss: 3.6323\n",
      "Epoch: 59, Loss: 3.6109\n",
      "Epoch: 60, Loss: 3.6139\n",
      "Epoch: 61, Loss: 3.6089\n",
      "Epoch: 62, Loss: 3.6096\n",
      "Epoch: 63, Loss: 3.5468\n",
      "Epoch: 64, Loss: 3.5750\n",
      "Epoch: 65, Loss: 3.5738\n",
      "Epoch: 66, Loss: 3.5420\n",
      "Epoch: 67, Loss: 3.5346\n",
      "Epoch: 68, Loss: 3.5595\n",
      "Epoch: 69, Loss: 3.5812\n",
      "Epoch: 70, Loss: 3.5100\n",
      "Epoch: 71, Loss: 3.5106\n",
      "Epoch: 72, Loss: 3.5095\n",
      "Epoch: 73, Loss: 3.4853\n",
      "Epoch: 74, Loss: 3.4559\n",
      "Epoch: 75, Loss: 3.4283\n",
      "Epoch: 76, Loss: 3.4612\n",
      "Epoch: 77, Loss: 3.4393\n",
      "Epoch: 78, Loss: 3.4271\n",
      "Epoch: 79, Loss: 3.4236\n",
      "Epoch: 80, Loss: 3.4602\n",
      "Epoch: 81, Loss: 3.4703\n",
      "Epoch: 82, Loss: 3.4098\n",
      "Epoch: 83, Loss: 3.4546\n",
      "Epoch: 84, Loss: 3.3625\n",
      "Epoch: 85, Loss: 3.3767\n",
      "Epoch: 86, Loss: 3.3465\n",
      "Epoch: 87, Loss: 3.3925\n",
      "Epoch: 88, Loss: 3.4200\n",
      "Epoch: 89, Loss: 3.3687\n",
      "Epoch: 90, Loss: 3.3598\n",
      "Epoch: 91, Loss: 3.3612\n",
      "Epoch: 92, Loss: 3.3606\n",
      "Epoch: 93, Loss: 3.3694\n",
      "Epoch: 94, Loss: 3.4284\n",
      "Epoch: 95, Loss: 3.3324\n",
      "Epoch: 96, Loss: 3.2846\n",
      "Epoch: 97, Loss: 3.3172\n",
      "Epoch: 98, Loss: 3.2938\n",
      "Epoch: 99, Loss: 3.3010\n"
     ]
    }
   ],
   "source": [
    "steps = 100\n",
    "batch_size = 32\n",
    "\n",
    "for step in range(steps):\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    xb, yb = get_batch(train_ids, subkey, batch_size, block_size)\n",
    "\n",
    "    loss, grads = value_and_grad(loss_fn, argnums=(0))(\n",
    "        variables, \n",
    "        xb, \n",
    "        yb\n",
    "    )\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, variables)\n",
    "    variables = optax.apply_updates(variables, updates) \n",
    "\n",
    "    print(f\"Epoch: {step}, Loss: {loss :.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      "\n",
      "P?NLp,IRUmvt&UnpeAqodwb\n",
      "ky;JkDLORenmgrkn,Pm, owraSle-nsVit;b3k!haugy wt:!MI',\n",
      "YLDGLnicbbunbeG'T?UvbL\n"
     ]
    }
   ],
   "source": [
    "index_seq = jnp.zeros(shape=(1,1), dtype=jnp.uint16)\n",
    "max_new_tokens = 100\n",
    "\n",
    "generated_indices = generate(variables, index_seq, rng_key, vocab_size, 1, max_new_tokens)\n",
    "generated_indices = list(np.array(generated_indices[0]))\n",
    "print(\"Generated text: \")\n",
    "print(decode(generated_indices))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flax2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61a659281f728d55fc033691f61c254a0324a8156d8c726d9ae327f7f6499663"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

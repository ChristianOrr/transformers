{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Language Model\n",
    "\n",
    "We will build an autoregressive bigram language model for predicting the next token in a sequence of text. The shakespeare_char dataset will be used for this demonstration, which can be found in the data folder.  \n",
    "\n",
    "Bigram is a probabilistic model. It uses the previous token in the sequence to determine the probabilities of the next tokens occuring. Then the next token is sampled using the next tokens probabilities. For further explanation of the bigram model, see Andrej Karpathy's video [2].\n",
    "\n",
    "The n-gram models are a more general case of the bigram model. They differ from bigram in that they use the last n-1 tokens in the sequence instead of just the last word. This enables them to see further back in the sentence to make their prediction. \n",
    "\n",
    "### References:\n",
    "- [1] [GPT colab notebook](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing)\n",
    "- [2] [Video: bigram language model, loss, generation](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1331s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from jax import value_and_grad\n",
    "\n",
    "from helper_funcs import get_batch, generate, loss_fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1,115,394\n",
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "# download the tiny shakespeare dataset\n",
    "input_file_path = os.path.join('./data/shakespeare_char/input.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")\n",
    "\n",
    "# get all the unique characters that occur in this text\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 1,003,854 tokens\n",
      "val has 111,540 tokens\n"
     ]
    }
   ],
   "source": [
    "# create the train and test splits\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode both to integers\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(decode(train_ids[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(128)\n",
    "\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "(4, 8)\n",
      "[[ 1 41 53 51 51 39 52 42]\n",
      " [47 41 46  1 40 63  1 58]\n",
      " [43  1 58 53  1 57 39 60]\n",
      " [58 43 56  5 42  1 46 47]]\n",
      "targets:\n",
      "(4, 8)\n",
      "[[41 53 51 51 39 52 42 43]\n",
      " [41 46  1 40 63  1 58 46]\n",
      " [ 1 58 53  1 57 39 60 43]\n",
      " [43 56  5 42  1 46 47 57]]\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch(train_ids, rng_key, batch_size, block_size)\n",
    "\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [1] the target: 41\n",
      "when input is [1, 41] the target: 53\n",
      "when input is [1, 41, 53] the target: 51\n",
      "when input is [1, 41, 53, 51] the target: 51\n",
      "when input is [1, 41, 53, 51, 51] the target: 39\n",
      "when input is [1, 41, 53, 51, 51, 39] the target: 52\n",
      "when input is [1, 41, 53, 51, 51, 39, 52] the target: 42\n",
      "when input is [1, 41, 53, 51, 51, 39, 52, 42] the target: 43\n",
      "when input is [47] the target: 41\n",
      "when input is [47, 41] the target: 46\n",
      "when input is [47, 41, 46] the target: 1\n",
      "when input is [47, 41, 46, 1] the target: 40\n",
      "when input is [47, 41, 46, 1, 40] the target: 63\n",
      "when input is [47, 41, 46, 1, 40, 63] the target: 1\n",
      "when input is [47, 41, 46, 1, 40, 63, 1] the target: 58\n",
      "when input is [47, 41, 46, 1, 40, 63, 1, 58] the target: 46\n",
      "when input is [43] the target: 1\n",
      "when input is [43, 1] the target: 58\n",
      "when input is [43, 1, 58] the target: 53\n",
      "when input is [43, 1, 58, 53] the target: 1\n",
      "when input is [43, 1, 58, 53, 1] the target: 57\n",
      "when input is [43, 1, 58, 53, 1, 57] the target: 39\n",
      "when input is [43, 1, 58, 53, 1, 57, 39] the target: 60\n",
      "when input is [43, 1, 58, 53, 1, 57, 39, 60] the target: 43\n",
      "when input is [58] the target: 43\n",
      "when input is [58, 43] the target: 56\n",
      "when input is [58, 43, 56] the target: 5\n",
      "when input is [58, 43, 56, 5] the target: 42\n",
      "when input is [58, 43, 56, 5, 42] the target: 1\n",
      "when input is [58, 43, 56, 5, 42, 1] the target: 46\n",
      "when input is [58, 43, 56, 5, 42, 1, 46] the target: 47\n",
      "when input is [58, 43, 56, 5, 42, 1, 46, 47] the target: 57\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Uses the previous token in the sequence to \n",
    "    determine the probabilities of the next token.\n",
    "    \"\"\"\n",
    "    vocab_size: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Each token directly reads off the logits for the next token from a lookup table\n",
    "        token_embedding_table = nn.Embed(num_embeddings=self.vocab_size, features=self.vocab_size)\n",
    "        logits = token_embedding_table(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "\n",
    "variables = model.init(rng_key, xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8, 65)\n"
     ]
    }
   ],
   "source": [
    "out = model.apply(variables, xb)\n",
    "print(out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      "\n",
      "FeRkiTvg.,jtMwetQ\n",
      "x;;zZFeVmFgOtyYaXqu,wzhj Sfh,i3rE.,rrkHm'PDy,sja33d&;K:,EEhIeMCNl zv;wZkPlNl.lqbbL\n"
     ]
    }
   ],
   "source": [
    "index_seq = jnp.zeros(shape=(1,1), dtype=jnp.uint16)\n",
    "max_new_tokens = 100\n",
    "\n",
    "generated_indices = generate(variables, model.apply, index_seq, rng_key, vocab_size, 1, block_size, max_new_tokens)\n",
    "generated_indices = list(np.array(generated_indices[0]))\n",
    "print(\"Generated text: \")\n",
    "print(decode(generated_indices))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adamw(learning_rate=1e-2)\n",
    "opt_state = optimizer.init(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 4.1996\n",
      "Epoch: 1, Loss: 4.1725\n",
      "Epoch: 2, Loss: 4.1578\n",
      "Epoch: 3, Loss: 4.1523\n",
      "Epoch: 4, Loss: 4.1439\n",
      "Epoch: 5, Loss: 4.1430\n",
      "Epoch: 6, Loss: 4.1282\n",
      "Epoch: 7, Loss: 4.1220\n",
      "Epoch: 8, Loss: 4.1064\n",
      "Epoch: 9, Loss: 4.1081\n",
      "Epoch: 10, Loss: 4.0991\n",
      "Epoch: 11, Loss: 4.0791\n",
      "Epoch: 12, Loss: 4.0501\n",
      "Epoch: 13, Loss: 4.0383\n",
      "Epoch: 14, Loss: 4.0466\n",
      "Epoch: 15, Loss: 4.0380\n",
      "Epoch: 16, Loss: 4.0254\n",
      "Epoch: 17, Loss: 4.0144\n",
      "Epoch: 18, Loss: 3.9974\n",
      "Epoch: 19, Loss: 3.9868\n",
      "Epoch: 20, Loss: 3.9660\n",
      "Epoch: 21, Loss: 3.9618\n",
      "Epoch: 22, Loss: 3.9420\n",
      "Epoch: 23, Loss: 3.9424\n",
      "Epoch: 24, Loss: 3.9269\n",
      "Epoch: 25, Loss: 3.9313\n",
      "Epoch: 26, Loss: 3.9343\n",
      "Epoch: 27, Loss: 3.9111\n",
      "Epoch: 28, Loss: 3.8979\n",
      "Epoch: 29, Loss: 3.8893\n",
      "Epoch: 30, Loss: 3.8613\n",
      "Epoch: 31, Loss: 3.8779\n",
      "Epoch: 32, Loss: 3.8776\n",
      "Epoch: 33, Loss: 3.8579\n",
      "Epoch: 34, Loss: 3.8241\n",
      "Epoch: 35, Loss: 3.8421\n",
      "Epoch: 36, Loss: 3.7989\n",
      "Epoch: 37, Loss: 3.7984\n",
      "Epoch: 38, Loss: 3.7727\n",
      "Epoch: 39, Loss: 3.8095\n",
      "Epoch: 40, Loss: 3.7699\n",
      "Epoch: 41, Loss: 3.7758\n",
      "Epoch: 42, Loss: 3.7518\n",
      "Epoch: 43, Loss: 3.7608\n",
      "Epoch: 44, Loss: 3.7639\n",
      "Epoch: 45, Loss: 3.7967\n",
      "Epoch: 46, Loss: 3.7790\n",
      "Epoch: 47, Loss: 3.6979\n",
      "Epoch: 48, Loss: 3.6790\n",
      "Epoch: 49, Loss: 3.7294\n",
      "Epoch: 50, Loss: 3.6593\n",
      "Epoch: 51, Loss: 3.6845\n",
      "Epoch: 52, Loss: 3.6597\n",
      "Epoch: 53, Loss: 3.6408\n",
      "Epoch: 54, Loss: 3.6358\n",
      "Epoch: 55, Loss: 3.6476\n",
      "Epoch: 56, Loss: 3.6428\n",
      "Epoch: 57, Loss: 3.6153\n",
      "Epoch: 58, Loss: 3.6264\n",
      "Epoch: 59, Loss: 3.6352\n",
      "Epoch: 60, Loss: 3.6024\n",
      "Epoch: 61, Loss: 3.5696\n",
      "Epoch: 62, Loss: 3.5952\n",
      "Epoch: 63, Loss: 3.5828\n",
      "Epoch: 64, Loss: 3.6007\n",
      "Epoch: 65, Loss: 3.5406\n",
      "Epoch: 66, Loss: 3.5791\n",
      "Epoch: 67, Loss: 3.5299\n",
      "Epoch: 68, Loss: 3.5420\n",
      "Epoch: 69, Loss: 3.5148\n",
      "Epoch: 70, Loss: 3.4850\n",
      "Epoch: 71, Loss: 3.5083\n",
      "Epoch: 72, Loss: 3.5028\n",
      "Epoch: 73, Loss: 3.4495\n",
      "Epoch: 74, Loss: 3.5013\n",
      "Epoch: 75, Loss: 3.4860\n",
      "Epoch: 76, Loss: 3.4396\n",
      "Epoch: 77, Loss: 3.5065\n",
      "Epoch: 78, Loss: 3.4448\n",
      "Epoch: 79, Loss: 3.4913\n",
      "Epoch: 80, Loss: 3.4139\n",
      "Epoch: 81, Loss: 3.4124\n",
      "Epoch: 82, Loss: 3.4104\n",
      "Epoch: 83, Loss: 3.4873\n",
      "Epoch: 84, Loss: 3.4424\n",
      "Epoch: 85, Loss: 3.4047\n",
      "Epoch: 86, Loss: 3.4148\n",
      "Epoch: 87, Loss: 3.4223\n",
      "Epoch: 88, Loss: 3.4121\n",
      "Epoch: 89, Loss: 3.4427\n",
      "Epoch: 90, Loss: 3.3595\n",
      "Epoch: 91, Loss: 3.3841\n",
      "Epoch: 92, Loss: 3.3818\n",
      "Epoch: 93, Loss: 3.3125\n",
      "Epoch: 94, Loss: 3.3538\n",
      "Epoch: 95, Loss: 3.3842\n",
      "Epoch: 96, Loss: 3.3404\n",
      "Epoch: 97, Loss: 3.3148\n",
      "Epoch: 98, Loss: 3.3367\n",
      "Epoch: 99, Loss: 3.3312\n"
     ]
    }
   ],
   "source": [
    "steps = 100\n",
    "batch_size = 32\n",
    "\n",
    "for step in range(steps):\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    xb, yb = get_batch(train_ids, subkey, batch_size, block_size)\n",
    "\n",
    "    loss, grads = value_and_grad(loss_fn, argnums=(0))(\n",
    "        variables, \n",
    "        model.apply,\n",
    "        xb, \n",
    "        yb\n",
    "    )\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, variables)\n",
    "    variables = optax.apply_updates(variables, updates) \n",
    "\n",
    "    print(f\"Epoch: {step}, Loss: {loss :.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation Post-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      "\n",
      "frmIIt;DeLINCZlGHk.lxir,Uqh-3QhhSRK$'eI uxnxcUKWD3nCyVWe,cCy\n",
      "PAgOLCwH;&B!;phtwftheD aqEtoKA$VvO:IM:s\n"
     ]
    }
   ],
   "source": [
    "index_seq = jnp.zeros(shape=(1,1), dtype=jnp.uint16)\n",
    "max_new_tokens = 100\n",
    "\n",
    "generated_indices = generate(variables, model.apply, index_seq, rng_key, vocab_size, 1, block_size, max_new_tokens)\n",
    "generated_indices = list(np.array(generated_indices[0]))\n",
    "print(\"Generated text: \")\n",
    "print(decode(generated_indices))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
